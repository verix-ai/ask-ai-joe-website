

**When the Dead Speak: AI Brings New Voices to the Courtroom**

**Joe Finkelstein**

A courtroom is a place for facts, evidence, and the rule of law—but it’s also a stage for human emotion. Recently, a sentencing hearing took an unexpected and emotional turn when the sister of a murder victim used artificial intelligence to recreate her brother’s voice and face. Through a short video, he appeared to speak directly to the man convicted of killing him.

The words were written by his sister, but the voice, the cadence, even the facial expressions belonged to her brother. His image blinked, spoke, and seemed to make eye contact. The result? A gut punch of emotion that left some in the courtroom in tears—and others wondering: Are we ready for this?

This wasn’t science fiction or a movie preview. It was real life. And it’s part of a growing trend: families and prosecutors turning to AI tools to give victims a voice that feels eerily lifelike, even after death. It’s a powerful use of technology, but also a deeply controversial one.

Let’s unpack it.

### **What AI Can Do—And What It *Did* Here**

Using tools that mimic voice and generate lifelike avatars, the victim’s sister crafted a video as a kind of modern-day statement of impact. Traditionally, families read letters or speak to the court themselves to express how a crime has affected their lives. But in this case, she wanted her brother’s image to deliver the message.

The video wasn’t long, but it didn’t have to be. The victim, speaking through AI, forgave the defendant. He talked about what he had hoped for in life. He said goodbye. The courtroom fell silent.

Was this a beautiful tribute? A digital séance? A manipulation? That depends on who you ask.

### **A Tool for Healing or for Persuasion?**

There’s no doubt this video was cathartic for the victim’s family. They described it as a way of honoring their brother and bringing his personality back into the room one last time. But others raised concerns: Was it fair to the defendant? Did it blur the line between emotional testimony and emotional manipulation?

Imagine being on the receiving end of a message from someone you killed. Would that sway your judgment? What if you're a juror or a judge? Could it influence your sense of justice—not based on facts, but on emotional impact?

These are the ethical questions the legal world is now grappling with.

### **Other Ways AI Has Been Used in Memorializing the Dead**

Outside courtrooms, AI has already been used to help families “visit” with lost loved ones. In South Korea, a documentary showed a grieving mother interacting with a virtual version of her deceased daughter through a VR headset. In the U.S., an app called *HereAfter* lets people create AI versions of themselves to leave behind stories and messages for their families.

Another example: Holocaust museums have developed lifelike video interactions with survivors using AI. Visitors can ask questions and hear responses in the survivor’s own voice, thanks to training on hours of interviews and natural language programming.

All of this raises the same core issue: Just because we *can* do something with AI—should we?

### **Back in the Courtroom…**

Legally, there’s still a gray area. Victim impact statements have always had emotional weight. They’re not part of the trial’s fact-finding phase but the sentencing—after guilt has been determined. That gives them a bit more leeway. But no one’s quite sure how to handle AI-generated impact statements yet.

Some legal experts worry this opens the door to “deepfake justice,” where video evidence—real or synthetic—could skew a trial. Others believe it’s just the next evolution of storytelling, no different from showing a slideshow or playing a home video.

Where’s the line between honoring a victim and overstepping the bounds of fairness?

### **How the Tech Works**

To make this kind of video, a family can use programs that combine facial modeling, voice cloning, and natural language generation. Tools like D-ID, HeyGen, ElevenLabs, and Synthesia can take a photo or short clip, combine it with text or recorded speech, and produce a video that feels shockingly real.

You don’t need a Hollywood studio—just a laptop, a few samples, and some time. In the courtroom case, the family worked with a creative team to get everything just right. But the tools they used are publicly available.

That’s part of what makes this so powerful—and so scary.

### **Where We Go From Here**

Courts, like schools and hospitals, tend to lag behind technology. But AI is moving fast. Judges, attorneys, and lawmakers will soon have to develop standards for what’s allowed, what’s encouraged, and what crosses the line.

Will AI-generated victim statements be welcomed as therapeutic and meaningful? Or restricted for being too emotionally manipulative?

There are no easy answers. But here’s what I know: When I first saw the clip, I felt both awe and unease. The technology is impressive. The emotion is raw. But justice needs clarity, not confusion.

We can’t allow artificial intelligence to rewrite what’s real. But we also can’t ignore the ways it might help families heal.

In the end, it’s not just about what the dead say. It’s about who gets to speak for them—and how.

---

**Author's note:** Want to try a tool that generates speech or creates a video avatar? Check out:

* **ElevenLabs** for voice cloning

* **D-ID** or **HeyGen** for photo-to-video avatars

* **HereAfter** for creating memory-keeping voicebots

And if you're wondering whether AI belongs in your life, your job, or your courtroom...well, the jury’s still out.

